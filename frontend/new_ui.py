import os
import sys
import threading
from typing import Optional
import asyncio
from queue import Queue, Empty
import time
import warnings

import gradio as gr
import pynini
import onnxruntime as ort
from punctuators.models import PunctCapSegModelONNX

from api.services.chunkformer_stt import ChunkFormer
from api.private_config import *
from api.config import *          # ƒë·ªÉ l·∫•y SUMMARIZE_DOCUMENT_PROMPT, v.v.
from api.services.vcdb_faiss import VectorStore
from api.services.local_llm import LanguageModelOllama  # b·∫£n ƒë√£ c√≥ async_generate
from api.services.rag_processor import RagProcessor
from langchain_huggingface.embeddings import HuggingFaceEmbeddings

warnings.filterwarnings("ignore")

# =========================
# ITN MODEL
# =========================

def init_itn_model(itn_model_dir: str):
    print(f"Loading ITN model from: {itn_model_dir}")
    far_dir = os.path.join(itn_model_dir, "far")
    classifier_far = os.path.join(far_dir, "classify/tokenize_and_classify.far")
    verbalizer_far = os.path.join(far_dir, "verbalize/verbalize.far")

    if not (os.path.exists(classifier_far) and os.path.exists(verbalizer_far)):
        print(f"ERROR: Missing .far files in {far_dir}", file=sys.stderr)
        sys.exit(1)

    try:
        reader_classifier = pynini.Far(classifier_far)
        reader_verbalizer = pynini.Far(verbalizer_far)
        classifier = reader_classifier.get_fst()
        verbalizer = reader_verbalizer.get_fst()
        print("ITN model ready.")
        return classifier, verbalizer
    except Exception as e:
        print(f"Error loading ITN model: {e}", file=sys.stderr)
        sys.exit(1)


# =========================
# INIT GLOBAL MODELS
# =========================

chunkformer = ChunkFormer(model_checkpoint=CHUNKFORMER_CHECKPOINT)
punc_model = PunctCapSegModelONNX.from_pretrained(
    "1-800-BAD-CODE/xlm-roberta_punctuation_fullstop_truecase",
    ort_providers=["CPUExecutionProvider"],
)
itn_classifier, itn_verbalizer = init_itn_model(ITN_REPO)

# ---- RAG / LLM t·ª´ lu·ªìng c≈© ----
llm = LanguageModelOllama("shmily_006/Qw3:4b_4bit", temperature=0.5)
model_embedding = HuggingFaceEmbeddings(model_name=MODEL_EMBEDDING, model_kwargs={"trust_remote_code": True})

faiss = VectorStore("luat_hon_nhan_gia_dinh/documents", model_embedding)
transcript_faiss = VectorStore("luat_hon_nhan_gia_dinh/transcripts", model_embedding)
cache_faiss = VectorStore("luat_hon_nhan_gia_dinh/cache", model_embedding)

meeting_document_summarize = """LU·∫¨T H√îN NH√ÇN V√Ä GIA ƒê√åNH: Lu·∫≠t n√†y quy ƒë·ªãnh ch·∫ø ƒë·ªô h√¥n nh√¢n v√† gia ƒë√¨nh; chu·∫©n m·ª±c ph√°p l√Ω cho c√°ch ·ª©ng x·ª≠ gi·ªØa c√°c th√†nh vi√™n
gia ƒë√¨nh; tr√°ch nhi·ªám c·ªßa c√° nh√¢n, t·ªï ch·ª©c, Nh√† n∆∞·ªõc v√† x√£ h·ªôi trong vi·ªác x√¢y d·ª±ng, c·ªßng c·ªë ch·∫ø ƒë·ªô h√¥n
nh√¢n v√† gia ƒë√¨nh."""

# =========================
# RAG QUEUES & GLOBALS (lu·ªìng c≈©)
# =========================
job_queue = Queue(maxsize=0)
summarizer_queue = Queue(maxsize=0)

# =========================
# EMBEDDING QUEUE & LOCK (FAISS)
# =========================
embedding_queue = Queue(maxsize=0)
faiss_lock = threading.Lock()

# =========================
# ASYNC EVENT LOOP THREAD (lu·ªìng c≈©)
# =========================
_ASYNC_LOOP: asyncio.AbstractEventLoop | None = None
_ASYNC_THREAD: threading.Thread | None = None

def _loop_worker(loop: asyncio.AbstractEventLoop):
    asyncio.set_event_loop(loop)
    loop.run_forever()

def start_async_loop():
    global _ASYNC_LOOP, _ASYNC_THREAD
    if _ASYNC_LOOP is None:
        _ASYNC_LOOP = asyncio.new_event_loop()
        _ASYNC_THREAD = threading.Thread(target=_loop_worker, args=(_ASYNC_LOOP,), daemon=True)
        _ASYNC_THREAD.start()

def stop_async_loop():
    global _ASYNC_LOOP
    if _ASYNC_LOOP and _ASYNC_LOOP.is_running():
        _ASYNC_LOOP.call_soon_threadsafe(_ASYNC_LOOP.stop)

def run_async(coro, timeout: float | None = None):
    """
    Submit coroutine to the background loop from any thread and wait for result.
    """
    fut = asyncio.run_coroutine_threadsafe(coro, _ASYNC_LOOP)
    return fut.result(timeout=timeout)

# Kh·ªüi ƒë·ªông event loop n·ªÅn NGAY t·ª´ ƒë·∫ßu
start_async_loop()

# =========================
# T√ìM T·∫ÆT: prompt builder (lu·ªìng c≈©)
# =========================
def build_summary_prompt(utterance: str, docs) -> str:
    return SUMMARIZE_DOCUMENT_PROMPT.format(utterance=utterance, related_docs=docs)

# =========================
# GLOBAL STATE CHO UI M·ªöI
# =========================
asr_thread: Optional[threading.Thread] = None
stop_event = threading.Event()
transcript_lock = threading.Lock()

# formatted transcript (Punc + ITN)
transcript_text = ""   # c·ªôt tr√°i: script cu·ªôc h·ªçp
commit_log = ""        # log new_commit (backend d√πng n·∫øu c·∫ßn)

# summary t·ª´ RAG (c·ªôt ph·∫£i)
summary_lock = threading.Lock()
summary_text = ""      # c·ªôt ph·∫£i: summarize docs / t√≥m t·∫Øt

# buffer ƒë·ªÉ gom new_commit tr∆∞·ªõc khi ƒë·∫©y sang RAG
# rag_buffer = []
# rag_buffer_lock = threading.Lock()

rag_processor = RagProcessor(
    job_queue=job_queue,
    embedding_queue=embedding_queue,
    n_commits_to_combine=3,   # s·ªë commit gom th√†nh 1 chunk
    overlap_m=1,              # overlap gi·ªØa c√°c chunk
    timeout_sec=15.0,          # im l·∫∑ng 15s th√¨ flush m·ªõ c√≤n l·∫°i
    on_emit=None,             # ho·∫∑c truy·ªÅn callback ƒë·ªÉ debug n·∫øu th√≠ch
)

# =========================
# WORKER CH√çNH CHO RAG (lu·ªìng c≈©)
# =========================
def worker_loop(worker_id: int):
    print(f"[Worker-{worker_id}] Starting")
    while True:
        try:
            text = job_queue.get(timeout=1.0)
        except Empty:
            continue

        try:
            # 1) Chu·∫©n ho√° b·∫±ng async_generate (non-stream) ch·∫°y tr√™n loop n·ªÅn
            normalize_prompt = llm.normalize_text(meeting_document_summarize, text)
            normalized = run_async(llm.async_generate(normalize_prompt), timeout=60.0)
            print(f"[Worker-{worker_id}] C√¢u ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a v√† t·ªëi ∆∞u:", text, "\n", normalized)
            print("___________________________________________________________________________________________________________")

            if not normalized or normalized.strip().casefold() == "none":
                continue

            # Check cache: ƒë√£ x·ª≠ l√Ω ch∆∞a? ---
            if cache_faiss.is_already_retrieved(normalized, similarity_threshold=0.7):
                print(f"[Worker-{worker_id}] Skipped cached text: {normalized}")
                continue

            # 2) Retrieve t√†i li·ªáu li√™n quan (async ‚Üí d√πng run_async)
            related_docs = run_async(faiss.hybrid_search(normalized), timeout=60.0)

            # 3) ƒê·∫©y sang summarizer_queue ƒë·ªÉ t√≥m t·∫Øt song song
            summarizer_queue.put({
                "utterance": normalized,
                "related_docs": related_docs,
                "ts": time.time()
            })

            with faiss_lock:
                cache_faiss.add_cache(normalized)

        except Exception as e:
            print(f"[Worker-{worker_id}] ERROR processing job: {e}")
        finally:
            job_queue.task_done()

def start_workers(num_workers: int = 2):
    for i in range(num_workers):
        t = threading.Thread(target=worker_loop, args=(i+1,), daemon=True)
        t.start()

# =========================
# EMBEDDING WORKER (FAISS)
# =========================
def embedding_worker():
    """
    Worker chuy√™n nh·∫≠n item t·ª´ embedding_queue v√† nh√©t v√†o FAISS.
    Item c√≥ th·ªÉ l√† str ho·∫∑c dict {"text": ..., "start_time_ms": ..., "end_time_ms": ...}
    D√πng VectorStore.add_transcript(transcript, start, end, metadata).
    """
    print("[EmbeddingWorker] Starting")
    while True:
        try:
            item = embedding_queue.get(timeout=1.0)
        except Empty:
            continue

        try:
            # Extract text v√† timestamps t·ª´ item
            if isinstance(item, dict):
                clean_text = (item.get("text") or "").strip()
                start_ms = item.get("start_time_ms")
                end_ms = item.get("end_time_ms")
            else:
                clean_text = (item or "").strip()
                start_ms = 0
                end_ms = 0

            if not clean_text:
                continue

            with faiss_lock:
                transcript_faiss.add_transcript(clean_text, start_ms, end_ms)

            print("[EmbeddingWorker] Added transcript chunk to FAISS:", clean_text[:80],
                  f"start={start_ms}ms, end={end_ms}ms", "...")
        except Exception as e:
            print(f"[EmbeddingWorker] ERROR: {e}")
        finally:
            try:
                embedding_queue.task_done()
            except Exception:
                pass


def start_embedding_worker():
    t = threading.Thread(target=embedding_worker, daemon=True)
    t.start()

# =========================
# SUMMARIZER LOOP (song song, lu·ªìng c≈©)
# =========================
def summarizer_loop():
    global summary_text
    print("[Summarizer] Starting")
    while True:
        try:
            item = summarizer_queue.get(timeout=1.0)
        except Empty:
            continue

        try:
            utter = item.get("utterance", "")
            docs = item.get("related_docs", [])

            # Build prompt v√† g·ªçi async_generate tr√™n loop n·ªÅn
            sum_prompt = build_summary_prompt(utter, docs)
            summary = run_async(llm.async_generate(sum_prompt), timeout=60.0)

            # C·∫≠p nh·∫≠t v√πng summary cho UI (c·ªôt ph·∫£i)
            with summary_lock:
                if summary_text:
                    summary_text = (
                        summary_text
                        + "\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  NEW SUMMARY  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
                        + summary.strip()
                    )
                else:
                    summary_text = summary.strip()

            # (t√πy ch·ªçn) log ra console
            print("\n================= [SUMMARY] =================")
            print(summary.strip())
            print("=============================================\n")

        except Exception as e:
            print(f"[Summarizer] ERROR: {e}")
        finally:
            try:
                summarizer_queue.task_done()
            except Exception:
                pass

def start_summarizer():
    t = threading.Thread(target=summarizer_loop, daemon=True)
    t.start()

# Kh·ªüi ƒë·ªông workers & summarizer
start_workers(num_workers=2)
start_summarizer()
start_embedding_worker()


# =========================
# CALLBACK FROM CHUNKFORMER (lu·ªìng m·ªõi + RAG)
# =========================
def on_update(event: str, payload: dict):
    """
    Callback t·ª´ chunkformer_asr_realtime_punc_norm:

      - event = "partial":
            payload: {"display", "committed", "active"}
      - event = "commit":
            payload: {"new_commit", "committed", "display"}
      - event = "final_flush":
            payload: {"text"}
    """
    global transcript_text, commit_log

    with transcript_lock:
        if event == "partial":
            display = (payload.get("display") or "").strip()
            if display:
                transcript_text = display

        elif event == "commit":
            # update transcript (∆∞u ti√™n display n·∫øu c√≥, fallback committed)
            display = (payload.get("display")
                       or payload.get("committed")
                       or "").strip()
            if display:
                transcript_text = display
            # L·∫•y new_commit ƒë·ªÉ log (backend) + gom v√†o chunk 2-c√¢u tr∆∞·ªõc khi ƒë·∫©y sang RAG
            new_commit = (payload.get("new_commit") or "").strip()
            if new_commit:
                if commit_log:
                    commit_log_val = f"{commit_log}\n{new_commit}"
                else:
                    commit_log_val = new_commit
                commit_log = commit_log_val

                # enqueue_rag_with_overlap(new_commit, 3, 1)
                rag_processor.process_new_commit(payload)

        elif event == "final_flush":
            text = (payload.get("text") or "").strip()
            if text:
                transcript_text = text

            rag_processor.flush_all(reason="final_flush")
                # try:
                #     job_queue.put_nowait(text)
                # except Exception as e:
                #     print(f"[on_update] Cannot enqueue final text to job_queue: {e}")


# =========================
# ASR WORKER (lu·ªìng m·ªõi)
# =========================
def asr_worker():
    """
    Ch·∫°y tr√™n server, ƒë·ªçc mic local qua chunkformer_asr_realtime_punc_norm
    (ƒë√£ t√≠ch h·ª£p VAD + Punc + ITN).
    D·ª´ng khi stop_event ƒë∆∞·ª£c set.
    """
    try:
        chunkformer.chunkformer_asr_realtime_punc_norm(
            mic_sr=16000,
            stream_chunk_sec=0.5,
            lookahead_sec=0.5,
            left_context_size=128,
            right_context_size=32,
            max_overlap_match=32,
            # VAD
            vad_threshold=0.01,
            vad_min_silence_blocks=2,
            # Punc + ITN
            punc_model=punc_model,
            punc_window_words=100,
            punc_commit_margin_words=50,
            itn_classifier=itn_classifier,
            itn_verbalizer=itn_verbalizer,
            # Control
            on_update=on_update,
            stop_event=stop_event,
            return_final=False,
        )
    except Exception as e:
        print("[ASR] Error:", e, file=sys.stderr)


# =========================
# GRADIO CALLBACKS
# =========================
def start_asr():
    """
    Start button:
      - reset transcript + summary
      - clear stop_event
      - spawn asr_worker thread n·∫øu ch∆∞a ch·∫°y
    """
    global asr_thread, transcript_text, commit_log, summary_text
    with transcript_lock:
        transcript_text = ""
        commit_log = ""
    with summary_lock:
        summary_text = ""

    stop_event.clear()

    if asr_thread is None or not asr_thread.is_alive():
        t = threading.Thread(target=asr_worker, daemon=True)
        t.start()
        asr_thread = t
        return (
            gr.update(value=""),  # transcript_box
            gr.update(value=""),  # summary_box
            "ƒêang l·∫Øng nghe üéß",   # status
        )
    else:
        return (
            gr.update(),          # transcript_box
            gr.update(),          # summary_box
            "ASR ƒë√£ ch·∫°y r·ªìi ‚úÖ",
        )


def stop_asr():
    """
    Stop button: set stop_event, worker s·∫Ω t·ª± tho√°t v√≤ng while.
    """
    stop_event.set()
    rag_processor.flush_all(reason="stop_button")
    return "ƒê√£ g·ª≠i t√≠n hi·ªáu d·ª´ng ‚èπÔ∏è"


def poll_ui():
    """
    ƒê∆∞·ª£c g·ªçi b·ªüi gr.Timer ƒë·ªÉ c·∫≠p nh·∫≠t UI ƒë·ªãnh k·ª≥.
    """
    with transcript_lock:
        txt = transcript_text
    with summary_lock:
        sumtxt = summary_text
    return gr.update(value=txt), gr.update(value=sumtxt)


# =========================
# CHATBOT HANDLER (c·ªôt gi·ªØa)
# =========================
def chat_qa(history, message):
    """
    Chatbot t·∫°m th·ªùi, d√πng llm.async_generate ƒë·ªÉ n√≥i chuy·ªán b√¨nh th∆∞·ªùng.
    - history: list[(user, assistant)]
    - message: c√¢u h·ªèi m·ªõi t·ª´ ng∆∞·ªùi d√πng
    """
    if not message:
        return history, ""

    # Build context t·ª´ history (n·∫øu c√≥)
    history = history or []
    history_str_parts = []
    for u, a in history[-3:]:
        history_str_parts.append(f"User: {u}\nAI: {a}")
    history_str = "\n\n".join(history_str_parts)

    reformulated_question = run_async(llm.reformulate_question(message, history_str, meeting_document_summarize), timeout=60.0)
    print(reformulated_question)
    try:
        if reformulated_question["type"] == 0:
            reply = run_async(llm.normal_qa_handler(reformulated_question["new_question"], history_str, meeting_document_summarize))

        else:
            related_docs = run_async(faiss.hybrid_search(reformulated_question["new_question"]),
                                        timeout=60.0)
            if transcript_faiss.db is not None:
                related_transcript = run_async(transcript_faiss.hybrid_search(reformulated_question["new_question"]),
                                            timeout=60.0)
                print("transcript db OK!")
            else:
                related_transcript = ""
                print("transcript db is None!")

            reply = run_async(llm.rag_qa_handler(reformulated_question["new_question"], history_str, meeting_document_summarize,
                                                 related_docs, related_transcript), timeout=60.0)


        if not reply:
            reply = "M√¨nh ƒëang g·∫∑p ch√∫t tr·ª•c tr·∫∑c n√™n ch∆∞a tr·∫£ l·ªùi ƒë∆∞·ª£c c√¢u n√†y üòÖ."
    except Exception as e:
        print(f"[Chatbot] ERROR: {e}")
        reply = "Chatbot ƒëang g·∫∑p l·ªói n·ªôi b·ªô, b·∫°n th·ª≠ h·ªèi l·∫°i sau m·ªôt ch√∫t nh√©."

    # C·∫≠p nh·∫≠t history cho Chatbot (format list[(user, assistant)])
    new_history = history + [(message, reply)]
    return new_history, ""  # clear √¥ input



# =========================
# BUILD UI (3 c·ªôt, t·ªëi gi·∫£n & full-height)
# =========================

with gr.Blocks() as demo:
    gr.Markdown("### üìù Realtime Meeting Assistant")

    # H√†ng n√∫t ƒëi·ªÅu khi·ªÉn
    with gr.Row():
        with gr.Column(scale=1):
            start_btn = gr.Button("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu ghi √¢m", variant="primary")
        with gr.Column(scale=1):
            stop_btn = gr.Button("‚èπÔ∏è D·ª´ng", variant="secondary")
        with gr.Column(scale=1):
            status = gr.Markdown("ƒêang idle‚Ä¶")

    # Ba c·ªôt ch√≠nh
    with gr.Row(elem_classes=["main-row"]):
        # C·ªôt tr√°i: Transcript
        with gr.Column(scale=2, elem_classes=["main-col"]):
            gr.Markdown("**Transcript cu·ªôc h·ªçp**")
            transcript_box = gr.Textbox(
                show_label=False,
                placeholder="Transcript",
                lines=37,
                interactive=False,
                elem_classes=["full-height-box"],
            )

        # C·ªôt gi·ªØa: Chatbot
        with gr.Column(scale=3, elem_classes=["main-col"]):
            gr.Markdown("**Chatbot h·ªèi ƒë√°p v·ªÅ cu·ªôc h·ªçp**")
            chatbot = gr.Chatbot(
                label="",
                elem_classes=["full-height-chatbot"], resizable=True, height=680
            )
            # √î nh·∫≠p + n√∫t g·ª≠i nh·ªè n·∫±m trong Textbox (submit_btn)
            chat_input = gr.Textbox(
                show_label=False,
                placeholder="ƒê·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung cu·ªôc h·ªçp...",
                lines=2,
                submit_btn=True,   # n√∫t g·ª≠i nh·ªè ·ªü trong textbox
            )

        # C·ªôt ph·∫£i: Summaries / Docs
        with gr.Column(scale=2, elem_classes=["main-col"]):
            gr.Markdown("**T√≥m t·∫Øt & t√†i li·ªáu li√™n quan**")
            summary_box = gr.Textbox(
                show_label=False,
                placeholder="C√°c ƒëo·∫°n t√≥m t·∫Øt t·ª´ RAG s·∫Ω hi·ªÉn th·ªã t·∫°i ƒë√¢y...",
                lines=37,
                interactive=False,
                elem_classes=["full-height-box"],
            )

    # Start: reset + ch·∫°y thread ASR
    start_btn.click(
        fn=start_asr,
        outputs=[transcript_box, summary_box, status],
    )

    # Stop: set stop_event
    stop_btn.click(
        fn=stop_asr,
        outputs=[status],
    )

    # Timer: c·∫≠p nh·∫≠t transcript & summary
    timer = gr.Timer(value=0.25, active=True)
    timer.tick(
        fn=poll_ui,
        outputs=[transcript_box, summary_box],
    )

    # Chatbot wiring: ch·ªâ d√πng submit c·ªßa Textbox
    chat_input.submit(
        fn=chat_qa,
        inputs=[chatbot, chat_input],
        outputs=[chatbot, chat_input],
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
    # stop_async_loop()
